{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Q1. Write a Python function that takes a paragraph as input and performs the following tasks step by step:\n",
        "\n",
        "Tokenize the paragraph into words.\n",
        "\n",
        "Remove punctuation and lowercase the words.\n",
        "\n",
        "Apply stemming.\n",
        "\n",
        "Apply lemmatization.\n",
        "\n",
        "# ðŸ›  Your Task:\n",
        "Use NLTK or spaCy for tokenization, stemming, and lemmatization.\n",
        "Print output at each stage so you can compare."
      ],
      "metadata": {
        "id": "vtI80eZWXR3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample = \"\"\"\\\"Natural Language Processing (NLP) is amazing!\\\" said Sarah.\n",
        "\\\"But... how does it actually work?\\\" she wondered.\n",
        "Well, it's not magicâ€”it's math, data, and a lot of clever algorithms.\n",
        "With tools like NLTK, spaCy, and transformers, we can build chatbots, summarize articles, or even translate languages! Isnâ€™t that incredible?\"\"\"\n",
        "\n",
        "\n",
        "#Tokenization\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "tokenized_words=word_tokenize(sample)\n",
        "print('Tokenized Paragraph:',tokenized_words)\n",
        "\n",
        "\n",
        "#Lowercase + Punctuation\n",
        "import string\n",
        "cleaned_words=[word.lower() for word in tokenized_words if word not in string.punctuation]\n",
        "print('Lowercase + Punctuation:',cleaned_words)\n",
        "\n",
        "#Stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer=PorterStemmer()\n",
        "stemmed_words=[stemmer.stem(word) for word in cleaned_words]\n",
        "print('Stemming:',stemmed_words)\n",
        "\n",
        "#Lemmatization\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lematizer=WordNetLemmatizer()\n",
        "lemmatized_words=[lematizer.lemmatize(word) for word in cleaned_words]\n",
        "print('Lemmatization:',lemmatized_words)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoduC3-_Xn5I",
        "outputId": "6d6e343b-7d40-47cb-f3f7-3adb64c34ee8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized Paragraph: ['``', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'amazing', '!', \"''\", 'said', 'Sarah', '.', '``', 'But', '...', 'how', 'does', 'it', 'actually', 'work', '?', \"''\", 'she', 'wondered', '.', 'Well', ',', 'it', \"'s\", 'not', 'magicâ€”it', \"'s\", 'math', ',', 'data', ',', 'and', 'a', 'lot', 'of', 'clever', 'algorithms', '.', 'With', 'tools', 'like', 'NLTK', ',', 'spaCy', ',', 'and', 'transformers', ',', 'we', 'can', 'build', 'chatbots', ',', 'summarize', 'articles', ',', 'or', 'even', 'translate', 'languages', '!', 'Isn', 'â€™', 't', 'that', 'incredible', '?']\n",
            "Lowercase + Punctuation: ['``', 'natural', 'language', 'processing', 'nlp', 'is', 'amazing', \"''\", 'said', 'sarah', '``', 'but', '...', 'how', 'does', 'it', 'actually', 'work', \"''\", 'she', 'wondered', 'well', 'it', \"'s\", 'not', 'magicâ€”it', \"'s\", 'math', 'data', 'and', 'a', 'lot', 'of', 'clever', 'algorithms', 'with', 'tools', 'like', 'nltk', 'spacy', 'and', 'transformers', 'we', 'can', 'build', 'chatbots', 'summarize', 'articles', 'or', 'even', 'translate', 'languages', 'isn', 'â€™', 't', 'that', 'incredible']\n",
            "Stemming: ['``', 'natur', 'languag', 'process', 'nlp', 'is', 'amaz', \"''\", 'said', 'sarah', '``', 'but', '...', 'how', 'doe', 'it', 'actual', 'work', \"''\", 'she', 'wonder', 'well', 'it', \"'s\", 'not', 'magicâ€”it', \"'s\", 'math', 'data', 'and', 'a', 'lot', 'of', 'clever', 'algorithm', 'with', 'tool', 'like', 'nltk', 'spaci', 'and', 'transform', 'we', 'can', 'build', 'chatbot', 'summar', 'articl', 'or', 'even', 'translat', 'languag', 'isn', 'â€™', 't', 'that', 'incred']\n",
            "Lemmatization: ['``', 'natural', 'language', 'processing', 'nlp', 'is', 'amazing', \"''\", 'said', 'sarah', '``', 'but', '...', 'how', 'doe', 'it', 'actually', 'work', \"''\", 'she', 'wondered', 'well', 'it', \"'s\", 'not', 'magicâ€”it', \"'s\", 'math', 'data', 'and', 'a', 'lot', 'of', 'clever', 'algorithm', 'with', 'tool', 'like', 'nltk', 'spacy', 'and', 'transformer', 'we', 'can', 'build', 'chatbots', 'summarize', 'article', 'or', 'even', 'translate', 'language', 'isn', 'â€™', 't', 'that', 'incredible']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Problem 2: POS Tag and N-Gram Explorer\n",
        "**ðŸ“˜ Question:**\n",
        "**Create a function that:**\n",
        "\n",
        "Tokenizes a given sentence\n",
        "\n",
        "Tags each token with its Part of Speech (POS)\n",
        "\n",
        "Extracts N-grams (bigrams & trigrams)\n",
        "\n",
        "# ðŸ›  Your Task:\n",
        "Use nltk.pos_tag() for POS tagging\n",
        "\n",
        "Use nltk.ngrams() or your own function to get bigrams/trigrams\n",
        "\n",
        "Print tokens with tags, bigrams, and trigrams"
      ],
      "metadata": {
        "id": "tUrK76ghiIF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag, ngrams\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "text=\"The little girl is playing in the garden.\"\n",
        "\n",
        "#Tokenization\n",
        "\n",
        "tokens=word_tokenize(text)\n",
        "print('Tokens:',tokens)\n",
        "\n",
        "#POS Tagging\n",
        "pos_tags=pos_tag(tokens)\n",
        "print('POS Tags:',pos_tags)\n",
        "\n",
        "#N-grams\n",
        "bi_grams=list(ngrams(tokens,2))\n",
        "tri_grams=list(ngrams(tokens,3))\n",
        "print('Bigrams:',bi_grams)\n",
        "print('Trigrams:',tri_grams)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkwBOWQziWWo",
        "outputId": "93071e80-bf43-40be-cfda-cadf1b242e8c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['The', 'little', 'girl', 'is', 'playing', 'in', 'the', 'garden', '.']\n",
            "POS Tags: [('The', 'DT'), ('little', 'JJ'), ('girl', 'NN'), ('is', 'VBZ'), ('playing', 'VBG'), ('in', 'IN'), ('the', 'DT'), ('garden', 'NN'), ('.', '.')]\n",
            "Bigrams: [('The', 'little'), ('little', 'girl'), ('girl', 'is'), ('is', 'playing'), ('playing', 'in'), ('in', 'the'), ('the', 'garden'), ('garden', '.')]\n",
            "Trigrams: [('The', 'little', 'girl'), ('little', 'girl', 'is'), ('girl', 'is', 'playing'), ('is', 'playing', 'in'), ('playing', 'in', 'the'), ('in', 'the', 'garden'), ('the', 'garden', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 3: TF-IDF Feature Extractor\n",
        "**ðŸ“˜ Question:**\n",
        "**Build a small search engine-like program that:**\n",
        "\n",
        "Takes a list of small documents (5â€“6 sentences each)\n",
        "\n",
        "Calculates TF-IDF scores for each word in the corpus\n",
        "\n",
        "For a given word, prints the top 3 documents where the word is most important (highest TF-IDF)\n",
        "\n",
        "# ðŸ›  Your Task:\n",
        "Use sklearn.feature_extraction.text.TfidfVectorizer\n",
        "\n",
        "Accept a search word and sort the scores across documents\n",
        "\n",
        "Print top 3 matching documents"
      ],
      "metadata": {
        "id": "r1ubLwjajF9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "corpus = [\n",
        "    \"I love NLP and machine learning.\",\n",
        "    \"NLP is a subfield of AI.\",\n",
        "    \"Deep learning and NLP are connected.\",\n",
        "    \"Machine learning powers AI.\",\n",
        "    \"Natural language processing is NLP.\"\n",
        "]\n",
        "\n",
        "# tokens=[word_tokenize(doc) for doc in corpus]\n",
        "# single=[word for sublist in tokens for word in sublist]\n",
        "# clean= [token for token in single if token not in string.punctuation]\n",
        "# word_f=Counter(clean)\n",
        "# final= np.array(list(word_f.items()))\n",
        "# print(final)\n",
        "\n",
        "search_word = \"NLP\"\n",
        "vectorizer=TfidfVectorizer()\n",
        "tfidf_matrix=vectorizer.fit_transform(corpus)\n",
        "feature_names=vectorizer.get_feature_names_out()\n",
        "print(feature_names)\n",
        "print(tfidf_matrix.toarray())\n",
        "print(tfidf_matrix.shape)\n",
        "\n",
        "# Step 5: Check if the word is in the vocabulary\n",
        "if search_word.lower() not in feature_names:\n",
        "    print(f\"'{search_word}' not found in vocabulary.\")\n",
        "else:\n",
        "    # Step 6: Find the column index of the search word\n",
        "    word_index = np.where(feature_names == search_word.lower())[0][0]\n",
        "    print(word_index)\n",
        "\n",
        "    # Step 7: Extract the TF-IDF scores for this word across all documents\n",
        "    scores = tfidf_matrix[:, word_index].toarray().flatten()\n",
        "    print(scores)\n",
        "\n",
        "    # # Step 8: Sort and get top 3 indices\n",
        "    top_indices = scores.argsort()[::-1][:3]\n",
        "\n",
        "    # # Step 9: Print the top 3 documents\n",
        "    print(f\"Top 3 documents where '{search_word}' is most important:\\n\")\n",
        "    for i in top_indices:\n",
        "        if scores[i] > 0:\n",
        "            print(f\"Doc {i+1} (Score: {scores[i]:.4f}): {corpus[i]}\")\n",
        "        else:\n",
        "            print(f\"Doc {i+1}: (No relevance for '{search_word}')\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wju0lffhjP8-",
        "outputId": "181889d7-1db5-4f97-a4d6-1c7b40528e9f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ai' 'and' 'are' 'connected' 'deep' 'is' 'language' 'learning' 'love'\n",
            " 'machine' 'natural' 'nlp' 'of' 'powers' 'processing' 'subfield']\n",
            "[[0.         0.46063063 0.         0.         0.         0.\n",
            "  0.         0.38236504 0.5709398  0.46063063 0.         0.32165752\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.42408634 0.         0.         0.         0.         0.42408634\n",
            "  0.         0.         0.         0.         0.         0.29613871\n",
            "  0.52564409 0.         0.         0.52564409]\n",
            " [0.         0.38389033 0.47582217 0.47582217 0.47582217 0.\n",
            "  0.         0.31866365 0.         0.         0.         0.26806991\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.48648432 0.         0.         0.         0.         0.\n",
            "  0.         0.40382593 0.         0.48648432 0.         0.\n",
            "  0.         0.60298477 0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.40500406\n",
            "  0.50199209 0.         0.         0.         0.50199209 0.28281359\n",
            "  0.         0.         0.50199209 0.        ]]\n",
            "(5, 16)\n",
            "11\n",
            "[0.32165752 0.29613871 0.26806991 0.         0.28281359]\n",
            "Top 3 documents where 'NLP' is most important:\n",
            "\n",
            "Doc 1 (Score: 0.3217): I love NLP and machine learning.\n",
            "Doc 2 (Score: 0.2961): NLP is a subfield of AI.\n",
            "Doc 5 (Score: 0.2828): Natural language processing is NLP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U84bzgUalwIH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}